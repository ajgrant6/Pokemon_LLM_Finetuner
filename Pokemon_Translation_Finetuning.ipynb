{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ajgrant6/Pokemon_LLM_Finetuner/blob/main/Pokemon_Translation_Finetuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "67bTZrRRnPT0"
      },
      "outputs": [],
      "source": [
        "# !pip install datasets evaluate transformers[sentencepiece]\n",
        "# !pip install accelerate\n",
        "# !pip install scikit-learn\n",
        "# !pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d9zG0M5-naCn"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0H2WIe-wQZc"
      },
      "source": [
        "# The Problem\n",
        "Nintendo localizes the names of Pokemon, but a lot of translation systems fail to capture these localized names. I'd like to improve on these systems by finetuning them to understand these localized names."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUVGKbQBytIT"
      },
      "source": [
        "# The Dataset\n",
        "\n",
        "## Loading the Data\n",
        "\n",
        "Here is the source of the dataset:\n",
        "\n",
        " https://www.pokecommunity.com/threads/international-list-of-names-in-csv.460446/\n",
        "\n",
        " https://docs.google.com/spreadsheets/d/1Eo6oWs4RA5M4c0r9M8FXJniOyhpmNmrnULabkP8kbL8/edit?usp=sharing&source=pokecommunity.com\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n7wEYR2WwOpx"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# https://www.pokecommunity.com/threads/international-list-of-names-in-csv.460446/\n",
        "data = load_dataset(\"csv\", data_files = \"/content/PokemonNames.csv\")\n",
        "\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rid3EFxWHnpf"
      },
      "outputs": [],
      "source": [
        "data[\"train\"][0][\"en\"]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop columns except for en and de\n",
        "\n",
        "# Select the 'train' split\n",
        "data_train = data['train']\n",
        "\n",
        "# Remove all columns except 'en' and 'de'\n",
        "columns_to_keep = ['en', 'de']\n",
        "columns_to_remove = [col for col in data_train.column_names if col not in columns_to_keep]\n",
        "filtered_train = data_train.remove_columns(columns_to_remove)\n",
        "\n",
        "# Replace the original train split with the filtered one\n",
        "data['train'] = filtered_train\n",
        "\n",
        "data"
      ],
      "metadata": {
        "id": "zDUZHtGgyand"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NJe17kU1a-J"
      },
      "source": [
        "# The Model\n",
        "\n",
        "## Loading the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s1dTFkmW1hkE"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# We're gonna use the Helsinki English to German model\n",
        "model_checkpoint = \"Helsinki-NLP/opus-mt-en-de\"\n",
        "\n",
        "translator = pipeline(\"translation\", model = model_checkpoint)\n",
        "translator(\"Bulbasaur\")\n",
        "\n",
        "# The correct translation for \"Bulbasaur\" is \"Bisasam\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3iI2DcC1rHn"
      },
      "source": [
        "## Loading the Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eli6KPYW1jyb"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, return_tensors=\"pt\")\n",
        "\n",
        "en_name = data[\"train\"][0][\"en\"]\n",
        "de_name = data[\"train\"][0][\"de\"]\n",
        "\n",
        "inputs = tokenizer(en_name, text_target=de_name)\n",
        "inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZR6EHmNA2vxG"
      },
      "outputs": [],
      "source": [
        "tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"])\n",
        "# tokenizer.convert_ids_to_tokens(inputs[\"labels\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aX7-zj9j3jco"
      },
      "source": [
        "## Preprocess Function\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iL6YJH7HK6tz"
      },
      "outputs": [],
      "source": [
        "def preprocess_function(examples):\n",
        "    inputs = examples[\"en\"]\n",
        "    targets = examples[\"de\"]\n",
        "    model_inputs = tokenizer(\n",
        "        inputs, text_target=targets, max_length=max_length, truncation=True\n",
        "    )\n",
        "    return model_inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lD5MgISkLgoR"
      },
      "source": [
        "## Tokenization of the Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SrXzbXiMGib"
      },
      "source": [
        "# Finetuning\n",
        "## Setting up the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VF82wdqa50Jr"
      },
      "outputs": [],
      "source": [
        "max_length = 16 # Pokemon names are usually short\n",
        "\n",
        "tokenized_datasets = data.map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        "    remove_columns = data[\"train\"].column_names,\n",
        ")\n",
        "\n",
        "tokenized_datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bezOVVHfWwbu"
      },
      "outputs": [],
      "source": [
        "# Print a few examples from the tokenized dataset to inspect their structure\n",
        "for i in range(5):\n",
        "    print(f\"Example {i}: {tokenized_datasets['train'][i]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YF_C3lixMkb4"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSeq2SeqLM\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KC0XwNPaMt6f"
      },
      "source": [
        "## Data Collator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BO7h0C56Mvi3"
      },
      "outputs": [],
      "source": [
        "from transformers import DataCollatorForSeq2Seq\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
        "\n",
        "batch = data_collator([tokenized_datasets[\"train\"][i] for i in range(1, 3)])\n",
        "# batch.keys()\n",
        "# batch[\"decoder_input_ids\"]\n",
        "# batch[\"labels\"]\n",
        "\n",
        "for i in range(0, 3):\n",
        "    print(tokenized_datasets[\"train\"][i][\"labels\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhuG9n0zOGpM"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZEwqhTE6OaP0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from datasets import load_metric\n",
        "import evaluate\n",
        "\n",
        "# Load the metric (e.g., BLEU)\n",
        "metric = evaluate.load(\"exact_match\")\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    preds, labels = eval_preds\n",
        "    if isinstance(preds, tuple):\n",
        "        preds = preds[0]\n",
        "\n",
        "    # Decode predictions\n",
        "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "\n",
        "    # Replace -100s in the labels as we can't decode them\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    # Post-process\n",
        "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
        "    decoded_labels = [label.strip() for label in decoded_labels]\n",
        "\n",
        "    # Compute the metric\n",
        "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "    return {\"exact_match\": result[\"exact_match\"]}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmlZF80oO-Xi"
      },
      "source": [
        "## Fine-Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nb5TmZL9OmOa"
      },
      "outputs": [],
      "source": [
        "from transformers import Seq2SeqTrainingArguments\n",
        "\n",
        "args = Seq2SeqTrainingArguments(\n",
        "    f\"pokemon-finetuned-opus-mt-en-de\",\n",
        "    evaluation_strategy=\"no\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=1e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=3,\n",
        "    num_train_epochs=10,\n",
        "    predict_with_generate=True,\n",
        "    fp16=True,\n",
        "    push_to_hub=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a1BaQt4GX7kQ"
      },
      "outputs": [],
      "source": [
        "print(tokenized_datasets[\"train\"][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k9OmYsFZPUgt"
      },
      "outputs": [],
      "source": [
        "from transformers import Seq2SeqTrainer\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"train\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mCRLVvLyRpR2"
      },
      "outputs": [],
      "source": [
        "pre_tune_score = trainer.evaluate()\n",
        "pre_tune_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDmuaS6XR4Vi"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AQYAWKVqSMfk"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Qx9BXbmSOr_"
      },
      "outputs": [],
      "source": [
        "post_tune_score = trainer.evaluate(max_length=max_length)\n",
        "post_tune_score"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyMQzdnYsxZma3x3UFzrubM3",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}